# Synthetic Experiments

This experiment validates TNShap on synthetic multilinear functions where exact Shapley values can be computed, enabling rigorous evaluation of the method's accuracy.

## Overview

The synthetic experiments provide controlled validation of TNShap by:
- Generating synthetic multilinear functions with known Shapley values
- Training tensor network students to mimic these functions
- Computing TNShap estimates and comparing to ground truth
- Studying the impact of tensor network rank and architecture

## Key Scripts

### Main Experiment Scripts
- `synthetic_rank_sweep_basic.py` - Basic rank sweep experiments
- `synthetic_rank_sweep_multi_seed.py` - Multi-seed robustness study
- `synthetic_rank_sweep_enhanced.py` - Enhanced experiments with more metrics
- `synthetic_validation_framework.py` - Core synthetic experiment framework

### Analysis Scripts
- `synthetic_focused_experiments.py` - Focused experiments on specific configurations
- `synthetic_rank_loss_ablation.py` - Rank and loss function ablations
- `synthetic_evaluate_all_orders.py` - Comprehensive order evaluation
- `synthetic_generate_figures.py` - Generate publication figures and tables

### Suite Scripts
- `synthetic_experiment_suite.py` - Run complete synthetic experiment suite

## Experimental Design

### Synthetic Function Types
1. **Generic Multilinear**: Random multilinear functions with known coefficients
2. **TensorTree Ground Truth**: Functions generated by tensor networks (TN-to-TN learning)
3. **Sparse Multilinear**: Functions with sparse interaction patterns

### Tensor Network Configurations
- **Ranks**: 2, 3, 4, 5, 6, 8, 10, 12, 15
- **Architectures**: Balanced binary trees
- **Training**: Student networks trained to mimic teacher functions

### Evaluation Metrics
- **Training R²**: Student-teacher fit quality
- **TNShap R² Order 1**: Shapley value accuracy
- **TNShap R² Order 2**: Pairwise interaction accuracy  
- **TNShap R² Order 3**: Three-way interaction accuracy

## Key Findings

### Rank vs. Performance
- **Low Ranks (2-4)**: Good for simple functions, limited expressiveness
- **Medium Ranks (6-8)**: Sweet spot for most synthetic functions
- **High Ranks (10+)**: Diminishing returns, potential overfitting

### Function Type Impact
- **Generic Multilinear**: TNShap achieves R² > 0.95 for appropriate ranks
- **TensorTree GT**: Near-perfect accuracy (R² > 0.99) due to architectural match
- **Sparse Functions**: More challenging, requires careful rank selection

### Multi-Seed Robustness
- **Training Consistency**: Higher ranks show more consistent training
- **TNShap Stability**: Order 1 most stable, Order 3 most variable
- **Statistical Significance**: 10 seeds sufficient for robust conclusions

## Reproducing Results

### Quick Start
```bash
# Run basic rank sweep
python scripts/synthetic_rank_sweep_basic.py --seed 42

# Run multi-seed study
python scripts/synthetic_rank_sweep_multi_seed.py --n_seeds 10

# Run complete suite
python scripts/synthetic_experiment_suite.py
```

### Custom Experiments
```bash
# Test specific ranks
python scripts/synthetic_validation_framework.py \
    --ranks 2 4 6 8 \
    --function_type generic_multilinear \
    --seed 42

# Focused experiment
python scripts/synthetic_focused_experiments.py \
    --rank 6 \
    --function_type tensor_tree \
    --n_test_points 50
```

### Generate Figures
```bash
# Create publication figures
python scripts/synthetic_generate_figures.py

# Generate specific plots
python scripts/plot_tn_entanglement.py
```

## Results Structure

```
results/
├── results_teacher_student_multi_seed/    # Multi-seed robustness study
│   ├── summary_YYYYMMDD_HHMMSS.csv       # Mean ± std statistics
│   ├── detailed_YYYYMMDD_HHMMSS.csv      # Individual run results
│   └── README_experiment.md              # Auto-generated documentation
├── results_teacher_student_sweep/         # Rank sweep results
│   ├── generic_multilinear/              # Generic function results
│   ├── tensor_tree/                      # TN-to-TN results
│   └── sparse_multilinear/               # Sparse function results
└── figs/                                 # Generated figures
    ├── rank_ablation/                    # Rank vs performance plots
    ├── per_order_lines/                  # Order-specific results
    ├── scatters/                         # Scatter plots
    └── tables/                           # LaTeX tables
```

## Methodology

### Ground Truth Generation
1. **Generic**: Sample random multilinear coefficients
2. **TensorTree**: Generate function using known tensor network
3. **Sparse**: Create functions with limited interaction terms

### Student Training
1. **Data Generation**: Sample training points from teacher function
2. **Architecture**: Balanced binary tensor tree with specified rank
3. **Training**: SGD with early stopping, multiple seeds
4. **Evaluation**: Test on held-out points

### TNShap Computation
1. **Exact Shapley**: Compute ground truth using multilinear properties
2. **TNShap**: Use trained student network for estimation
3. **Comparison**: R² correlation between exact and estimated values

## Key Insights

1. **Architectural Match**: TN-to-TN learning achieves near-perfect accuracy
2. **Rank Selection**: Optimal rank depends on function complexity
3. **Robustness**: Multi-seed studies reveal training stability patterns
4. **Scalability**: Method works well across different function types

## Hardware Requirements

- **GPU**: Recommended for efficient training
- **Memory**: 8GB+ VRAM for rank 15 experiments
- **Storage**: ~10GB for complete synthetic results

## Statistical Analysis

### Confidence Intervals
With 10 seeds, approximate 95% confidence intervals:
```
CI = mean ± (1.96 * std / sqrt(10))
CI = mean ± (0.62 * std)
```

### Practical Significance
- **Training R²**: Differences > 0.02 likely practically significant
- **TNShap R²**: Differences > 0.05 likely practically significant

### Reporting Standards
Always report:
1. Number of runs (n=10)
2. Mean ± standard deviation
3. Seeds used for reproducibility
4. Hardware/software environment

## Future Directions

- Theoretical analysis of rank requirements
- Adaptive rank selection methods
- Multi-scale tensor network architectures
- Extension to non-multilinear functions
