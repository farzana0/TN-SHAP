# TNShap Methodology

This document provides a comprehensive overview of the TNShap methodology, including theoretical foundations, implementation details, and experimental design.

## Overview

TNShap (Tensor Network Shapley) is a method for computing Shapley values and higher-order feature interactions using tensor networks as surrogate models. The approach addresses the computational challenges of traditional Shapley value estimation by leveraging the efficiency of tensor network representations.

## Theoretical Foundations

### Shapley Values

Shapley values provide a principled way to attribute the contribution of each feature to a model's prediction. For a function $f: \mathbb{R}^d \to \mathbb{R}$ and input $x \in \mathbb{R}^d$, the Shapley value for feature $i$ is:

$$\phi_i(f, x) = \sum_{S \subseteq [d] \setminus \{i\}} \frac{|S|!(d-|S|-1)!}{d!} [f(x_S \cup \{i\}) - f(x_S)]$$

where $x_S$ denotes the input with features in set $S$ set to their baseline values.

### Higher-Order Interactions

Beyond first-order Shapley values, we can define higher-order interactions:

- **Order 1**: Individual feature contributions (Shapley values)
- **Order 2**: Pairwise feature interactions
- **Order 3**: Three-way feature interactions
- **Order k**: k-way feature interactions

### Computational Challenges

Traditional methods for computing Shapley values face several challenges:

1. **Exponential Complexity**: Exact computation requires $O(2^d)$ function evaluations
2. **Sampling Variance**: Monte Carlo methods introduce noise
3. **Scalability**: Limited to small feature spaces (d â‰¤ 20)

## Tensor Network Approach

### Binary Tensor Trees

TNShap uses binary tensor trees as surrogate models. A binary tensor tree represents a multilinear function as a hierarchical composition of tensors:

$$f(x) = \sum_{i_1, \ldots, i_d} T_{i_1, \ldots, i_d} x_1^{i_1} \cdots x_d^{i_d}$$

where $T$ is decomposed into a tree structure of smaller tensors.

### Architecture

The binary tensor tree architecture consists of:

1. **Leaf Nodes**: Input features with physical dimensions
2. **Internal Nodes**: Contraction tensors with bond dimensions
3. **Root Node**: Output tensor

### Feature Mapping

To handle non-multilinear functions, TNShap employs elementwise feature maps:

$$\psi: \mathbb{R} \to \mathbb{R}^m$$

where $m$ is the feature map output dimension. The feature map ensures $\psi(0) = 0$ for proper Shapley value computation.

## Implementation Details

### Forward Pass

The forward pass through a binary tensor tree involves:

1. **Feature Mapping**: Transform inputs using elementwise maps
2. **Tree Contraction**: Hierarchical tensor contractions
3. **Output**: Scalar or vector output

### Training Procedure

TNShap follows a teacher-student framework:

1. **Teacher Model**: Train a neural network on the original task
2. **Student Model**: Train a tensor network to mimic the teacher
3. **Shapley Computation**: Use the student for efficient Shapley estimation

### Shapley Value Computation

For tensor networks, Shapley values can be computed efficiently using the multilinear structure:

1. **Marginal Contributions**: Compute differences for each feature subset
2. **Weighted Sum**: Combine contributions with Shapley weights
3. **Efficient Evaluation**: Leverage tensor network structure for speed

## Experimental Design

### Synthetic Validation

Synthetic experiments validate TNShap on functions with known Shapley values:

1. **Multilinear Functions**: Random multilinear functions with exact Shapley values
2. **Tensor Tree Ground Truth**: Functions generated by tensor networks
3. **Sparse Functions**: Functions with limited interaction terms

### Real-World Evaluation

Real-world experiments evaluate TNShap on standard datasets:

1. **UCI Datasets**: Diabetes, Concrete, Energy, California Housing
2. **Baseline Comparison**: KernelSHAP, SHAPIQ, other methods
3. **Performance Metrics**: Accuracy, runtime, memory usage

### Ablation Studies

Systematic ablations study the impact of:

1. **Feature Map Dimensions**: Effect of $m$ on performance
2. **Tensor Network Ranks**: Optimal rank selection
3. **Architecture Choices**: Different tree structures

### Scaling Studies

Scaling experiments demonstrate:

1. **Dimensional Scalability**: Performance up to d=60
2. **Computational Efficiency**: Runtime and memory scaling
3. **Accuracy Preservation**: Maintaining quality at scale

## Key Advantages

### Computational Efficiency

1. **Polynomial Scaling**: $O(d \cdot r^2)$ vs $O(2^d)$ for exact methods
2. **GPU Acceleration**: Parallel tensor operations
3. **Memory Efficiency**: Structured parameter storage

### Accuracy

1. **Exact for Multilinear**: Perfect accuracy on multilinear functions
2. **Good Approximation**: High accuracy on general functions
3. **Robust**: Consistent performance across datasets

### Flexibility

1. **Architecture Agnostic**: Works with various tensor network structures
2. **Feature Mapping**: Handles non-multilinear functions
3. **Scalable**: Adapts to different problem sizes

## Limitations and Challenges

### Theoretical Limitations

1. **Approximation Error**: Not exact for general functions
2. **Rank Selection**: Optimal rank depends on function complexity
3. **Training Stability**: May require careful hyperparameter tuning

### Practical Challenges

1. **Memory Requirements**: Large models need significant VRAM
2. **Training Time**: Initial training can be expensive
3. **Hyperparameter Sensitivity**: Performance depends on good parameter choices

## Comparison with Baselines

### KernelSHAP

- **Advantage**: TNShap is faster and more accurate
- **Disadvantage**: Requires training a surrogate model

### SHAPIQ

- **Advantage**: TNShap scales to higher dimensions
- **Disadvantage**: SHAPIQ is exact for multilinear functions

### LIME

- **Advantage**: TNShap provides global explanations
- **Disadvantage**: LIME is simpler and more interpretable

## Future Directions

### Theoretical Developments

1. **Convergence Guarantees**: Theoretical analysis of approximation quality
2. **Rank Bounds**: Optimal rank selection criteria
3. **Error Analysis**: Bounds on Shapley value estimation error

### Methodological Improvements

1. **Adaptive Architectures**: Dynamic rank and structure selection
2. **Multi-Scale Methods**: Hierarchical feature interaction modeling
3. **Uncertainty Quantification**: Confidence intervals for Shapley estimates

### Applications

1. **High-Dimensional Data**: Genomics, imaging, NLP
2. **Real-Time Systems**: Online Shapley value computation
3. **Federated Learning**: Distributed Shapley value estimation

## Reproducibility

### Experimental Protocol

1. **Fixed Seeds**: Reproducible random initialization
2. **Multiple Runs**: Statistical significance testing
3. **Hardware Documentation**: System specifications recorded

### Code Availability

1. **Open Source**: Complete implementation available
2. **Documentation**: Comprehensive API and usage guides
3. **Examples**: Tutorial notebooks and scripts

### Data and Results

1. **Synthetic Data**: Generated with fixed seeds
2. **Public Datasets**: Standard UCI datasets
3. **Result Archives**: Complete experimental results

## Conclusion

TNShap provides a scalable and accurate approach to Shapley value estimation using tensor networks. The method addresses key limitations of traditional approaches while maintaining theoretical rigor and practical applicability. Future work will focus on theoretical analysis, methodological improvements, and broader applications.
